# train_for_image_recognition
# aim：
画像認識・物体検出モデルの精度および提供品質の向上（pytorchをベースに）

# how to do:
・現行モデルの課題を分析し、類似事例の対応手法を検証して精度向上を図る  
・最新の研究動向やOSS（オープンソースソフトウェア）を取り入れ、モデルの精度・パフォーマンス・再現性・安定性を継続的に改善する  
・実運用を見据えたモデルの設計・評価を行い、お客様の要件に応じた柔軟なカスタマイズを通じて納品品質を高める  

・現行モデルの構成を分解・整理し、初学者でも取り組みやすい単位でドキュメント化  
・精度低下の主な原因とその分析手法をまとめて、再現性のある検証プロセスを共有  
・精度向上の一般的な方法（データ拡張・正則化・ハイパーパラメータ調整など）を一覧化  
・分類タスクにおける流行モデルや設定の活用法を整理  
・物体検出・セグメンテーションモデルの知見を分類タスクへ応用可能な形でまとめる  

# issue:
## ・ datasetの準備 
## ・ 前処理（pre-processing） 
　- 推論の汎用性をアップするため、
 前処理入力画像を拡張する（randomapply(torchvision.transforms.ColorJitter(), GaussianBlur(kernel_size[, sigma])など)）
## ・ベースモデルの構造（effecientNet B0）  
## ・短時間に精度高いモデルを学習する方法  
### -hardware上（学習実行環境）での改善  
      ・dataloaderで利用するcpu（コア）の数num_workersを適切に選ぶ。例：lscpuでコア数（24）、16/1socketなので、16に設定した  
      ・pin_memory=Trueに設定した  
        →学習時間を40%短縮できた  
        
###  -学習の前処理での改善  
    ・transforms.GaussianBlur(kernel_size=11)→ transforms.RandomApply([transforms.GaussianBlur(kernel_size=11,sigma=(0.1, 2.0))], p=0.5)  
    →学習時間を短縮できた（40%?） 
    
###  -学習際のパラメータ調整  
    【1】optimizer（最適化アルゴリズム）の役割  
      ・勾配（gradient）を使って、モデルのパラメータ（重み）を更新する ものです。  
      ・loss.backward() で得られた勾配を基に、optimizer.step() によってパラメータを更新します。この更新に使われるのが「学習率（lr）」です  
      ・代表的な optimizer（PyTorch）
| Name  | feature |
| ------------- | ------------- |
| SGD | 最も基本的。勢い（momentum）を加えることで滑らかに進む  |
| Adam | 自動でlrを調整する要素あり（モーメントベース）。安定かつ速い  |
| RMSprop | 平均二乗勾配を使う。RNNでよく使われた  |
| Adagrad, AdamW など | 	特殊な性質を持つ学習向けにチューニングされている |  

    【2】scheduler（学習率スケジューラ）の役割  
    ・optimizerに設定された「学習率（lr）」を、トレーニング中に動的に調整する仕組み  
    ・学習初期：大きな学習率で速く学習  
      学習後期：小さな学習率で細かく調整（収束）  
      停滞：Plateauに入ったら lr を下げて再調整  
      一定周期で上げ下げ：脱局所最適を狙う
    
    【3】optimizer と scheduler の関係  
   | optimizer  | scheduler |
| ------------- | ------------- |
| モデルのパラメータを 更新する | その更新に使う「学習率（lr）」を 調整する  |
| optimizer.step() で重み更新 | scheduler.step() で lr を更新  |
| lr の初期値を持つ | その初期値から調整していく  |  

関係図イメージ：  
```text
          ┌─────────────┐
          │   scheduler │  ← 学習の進み具合に応じて
          └────┬────────┘
               ↓ lr を更新
          ┌────┴─────┐
          │ optimizer│  ← 勾配を使って重み更新
          └────┬─────┘
               ↓
          ┌────┴──────┐
          │  model    │ ← 重み更新される
          └───────────┘
```
###  -学習際のパラメータの解凍（fine-tuning)  
  ・EfficientNet（特にB0）の構造  
  | `features.i` | 層の構成                | 出力サイズ (入力224×224想定) | 抽出する特徴               | 解説                       |
| ------------ | ------------------- | ------------------- | -------------------- | ------------------------ |
| `features.0` | Conv3x3, stride=2   | 112×112             | **非常に低レベル**（エッジ、斑点）  | 最初の画像スキャン的処理（画像→特徴）      |
| `features.1` | MBConv1, k3x3, s=1  | 112×112             | 細かい模様、色              | channel数増やさず深さだけ追加       |
| `features.2` | MBConv6, k3x3, s=2  | 56×56               | 色の変化、局所パターン          | stride=2で空間解像度を半減        |
| `features.3` | MBConv6, k5x5, s=2  | 28×28               | 小さいパーツ、構成            | receptive field（受容野）が拡大  |
| `features.4` | MBConv6, k3x3, s=1  | 28×28               | シャリ＋ネタの相関            | 構造的特徴がより強くなる             |
| `features.5` | MBConv6, k5x5, s=2  | 14×14               | シャリ・ネタの組み合わせ         | 空間的にやや抽象的な関係性に注目         |
| `features.6` | MBConv6, k5x5, s=1  | 14×14               | ネタの種類（例：イカvsエビ）      | 空間解像度を維持しつつ表現力拡張         |
| `features.7` | MBConv6, k3x3, s=1  | 14×14               | クラス識別的な構造            | 形・輪郭など意味に近い特徴            |
| `features.8` | Conv1x1 → AvgPool2D | 7×7 → 1×1           | **最も抽象的**（クラス予測前の特徴） | classifierに渡される最後の特徴ベクトル |  

・観察ポイント（段階的fine-tuningで使える）  
🔸 features.0〜2：
画像の初期処理＋基本的な模様や色を捉える  
再利用性が非常に高く、ImageNetで学習済の重みがそのまま有効  
→ 最も凍結すべき層  

🔸 features.3〜6：  
中レベル：シャリとネタの配置、形状などを捉える  
類似クラス（まぐろ vs 中トロなど）の識別に貢献  
→ 中盤で解凍  

🔸 features.7〜8：  
最もクラス識別に近い層  
ここを寿司用に微調整することで精度が上がる  
→ 最初に解凍する層  

###  --学習の精度・時間に影響する主なパラメータ
| パラメータ                               | 精度への影響 | 時間への影響 | 説明                                                 |
| ----------------------------------- | ------ | ------ | -------------------------------------------------- |
| **1. 学習率（lr）**                      | ◎      | △（間接）  | 高すぎると発散、低すぎると収束遅い。適切なlrは学習精度を大きく左右します。             |
| **2. 解凍方法（全層 or 段階的）**              | ◎      | △〜○    | 段階的解凍は精度改善に寄与。全層解凍は時間がやや増えるが、早期に学習が進む可能性も。         |
| **3. DataLoaderのCPU数**              | ✕      | ◎      | 精度には無関係ですが、学習にかかる\*\*時間（データ読み込み速度）\*\*には大きく影響。     |
| **4. バッチサイズ**                       | △〜◎    | ◎      | 大きいと速いが精度が下がることも。小さいと精度は良くなることがあるが遅い。              |
| **5. オプティマイザの種類**                   | ○      | △      | Adam, SGD, RMSpropなどで収束特性や精度に差が出る。                 |
| **6. lrスケジューラ**                     | ◎      | △      | CosineAnnealing、StepLRなどで**安定した学習と最終精度に大きく影響**します。 |
| **7. 正則化（Weight Decay, Dropoutなど）** | ◎      | ✕      | 精度向上や過学習防止に重要。                                     |
| **8. データ拡張の有無・強さ**                  | ◎      | △      | 汎化性能に大きく寄与。ただし読み込みが遅くなることも。                        |  

例：300エポックにおける解凍スケジュール例（4段階解凍＋最終全層学習）  
| エポック範囲 | 解凍ステージ      | 備考         |
| ------ | ----------- | ---------- |
| 0〜19   | 解凍ステージ0（初期） | 最も浅い層のみ学習  |
| 20〜39  | 解凍ステージ1     | 中間層を一部解凍   |
| 40〜59  | 解凍ステージ2     | さらに中〜深部を解凍 |
| 60〜79  | 解凍ステージ3（全層） | 全層解凍開始     |
| 80〜299 | 解凍ステージ4（継続） | 全層状態で十分な学習 |　　

例：実用的な解凍戦略  
| 総epoch | classifierだけの期間 | 残り解凍フェーズ（4分割）           | 実際の解凍ポイント（目安）      |
| ------ | --------------- | ----------------------- | ------------------ |
| 100    | 最初の10epoch      | 残り90epochを 20epoch間隔で分割 | 10, 20, 30, 40     |
| 150    | 最初の15epoch      | 残り135を 4分割              | 15, 30, 45, 60     |
| 200    | 最初の20epoch      | 残り180を 4分割              | 20, 40, 60, 80     |
| 300    | 最初の20epoch      | 残り280を 4分割              | 20, 40, 60, 80     |
| 600    | 最初の50epoch      | 残り550を 4分割              | 50, 100, 150, 200  |
| 1000   | 最初の100epoch     | 残り900を 4分割              | 100, 200, 300, 400 |  





