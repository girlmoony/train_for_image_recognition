import torch
import torch.nn as nn

class WithPreproc(nn.Module):
    def __init__(self, backbone: nn.Module,
                 mean=(0.485, 0.456, 0.406),
                 std=(0.229, 0.224, 0.225),
                 expect_bgr=False):   # True にすると BGR→RGB を内部で実施
        super().__init__()
        self.backbone = backbone.eval()
        self.register_buffer("mean", torch.tensor(mean).view(1,3,1,1))
        self.register_buffer("std",  torch.tensor(std ).view(1,3,1,1))
        self.expect_bgr = expect_bgr

    def forward(self, x):  # x: NCHW, uint8 or float
        if x.dtype != torch.float32:
            x = x.float()
        # 0-255想定で来るなら 0-1 へ（もし既に0-1で来るなら下の1行は消す）
        x = x / 255.0
        # BGR入力ならRGBへ並べ替え（N, C, H, W）
        if self.expect_bgr:
            x = x[:, [2,1,0], :, :]
        # Normalize
        x = (x - self.mean) / self.std
        return self.backbone(x)



backbone = your_trained_model.eval()               # 学習済みEfficientNet-B0など
model = WithPreproc(backbone, mean=(0.485,0.456,0.406),
                    std=(0.229,0.224,0.225),
                    expect_bgr=False)              # ←外部はRGBで渡す場合
model.eval()

dummy = torch.zeros(1, 3, 256, 256, dtype=torch.uint8)  # 入力テンソルの想定形状/型
onnx_path = "effb0_with_preproc.onnx"

torch.onnx.export(
    model, dummy, onnx_path,
    opset_version=13,                 # 13以上推奨
    input_names=["input"],
    output_names=["logits"],
    dynamic_axes={"input": {0: "N"}, "logits": {0: "N"}},  # バッチ可変なら
)
print("saved:", onnx_path)


ツール側（キャリブ/量子化）の設定は「素通し」に

ラッパ内で Normalize/BGR→RGB をやるので、ツール側では一切前処理しない＝二重適用を防ぐ。

IN_DATA_FORMAT=1          # NCHW
IN_DATA_CHANNEL=3
IN_DATA_WIDTH=256
IN_DATA_HEIGHT=256

IS_BGR=0                  # ←ラッパが expect_bgr=False なら0（RGB受け）
IN_MEAN=0,0,0             # ←素通し
IN_SCALE=1                # ←素通し
PREPRO=NONE

# 出力ノード名はONNXをNetronで確認して設定
OUT_LAYER=logits
